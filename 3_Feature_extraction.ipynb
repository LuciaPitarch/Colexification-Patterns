{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Feature_extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOR7gKFiWZ2CyfWp89v4coB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuciaPitarch/Colexification-Patterns/blob/main/3_Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6VMzE7VGZfc"
      },
      "source": [
        "# 0. Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVA4ebnjGK3P"
      },
      "source": [
        "# Import libraries\n",
        "import pandas\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "# download pre-trained embedding model from nltk library\n",
        "nltk.download('word2vec_sample')\n",
        "path_to_word2vec_sample = nltk.data.find('models/word2vec_sample/pruned.word2vec.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYR-C0W-GUXS"
      },
      "source": [
        "# Import csv files\n",
        "!gdown --id 1GOhH20R9ToDnsoCqO0MjX0NUVSmZ2vqG #polynesian colexifications df\n",
        "!gdown --id 1HYTxiQMrpz_IYCsXoZIYFoH54mdkbixR #romance colexifications df\n",
        "!gdown --id 1YJz8XHfCn6_HGR7gbRwB2Ry2Z4ZSvYWK #swow_strength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yBbY0Z4Ggts"
      },
      "source": [
        "# Read files\n",
        "polynesian_df = pandas.read_csv('polynesian_df_colexifications.csv')\n",
        "romance_df = pandas.read_csv('romance_df_colexifications.csv')\n",
        "swow = pandas.read_csv('st_swow.csv', delimiter=(';'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45qllh0kGlcT"
      },
      "source": [
        "# 1. Cognitive features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNycCBUTG5ji"
      },
      "source": [
        "**1.1. Word similarity:**\n",
        "\n",
        "Word similarity measured by the cosine similarity between both colexified concepts. The word embeddings are retrieved from a pretrained model in word2vec accesible in the nltk library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0zq27tJGxUT"
      },
      "source": [
        "def lowercase (df):\n",
        "  df['Concepticon_Gloss.x'] = df['Concepticon_Gloss.x'].str.lower()\n",
        "  df['Concepticon_Gloss.y'] = df['Concepticon_Gloss.y'].str.lower()\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UNj_wZYGzF9"
      },
      "source": [
        "def add_colex_pairs (df):\n",
        "  df['pairs'] = list(zip(df['Concepticon_Gloss.x'], df['Concepticon_Gloss.y']))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6YNe0fFKmEt"
      },
      "source": [
        "#cosine similarity\n",
        "words = []\n",
        "vectors = []\n",
        "word2vec = {}\n",
        "with open(path_to_word2vec_sample) as file:\n",
        "    for i, line in enumerate(file):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        line = line.split()\n",
        "        word = line[0]\n",
        "        vector = line[1:]\n",
        "        vector = [float(s) for s in vector]\n",
        "        \n",
        "        words.append(word)\n",
        "        vectors.append(vector)\n",
        "        word2vec[word] = vector\n",
        "  \n",
        "def norm(vector):\n",
        "    return np.sqrt(sum([a*a for a in vector]))\n",
        "\n",
        "def dot(vector1, vector2):\n",
        "    return sum([i * j for i, j in zip(vector1, vector2)])\n",
        "\n",
        "def cosine(vector1, vector2):\n",
        "    return dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
        "\n",
        "def concept_to_embedding (concept1, concept2):\n",
        "  if concept1[0] in word2vec and concept2[0] in word2vec:\n",
        "    return cosine(word2vec[concept1[0]], word2vec[concept2[0]])\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def add_cosine_sim(df):\n",
        "  df['cosine_sim'] = df.apply(lambda row: concept_to_embedding([row['Concepticon_Gloss.x']], \n",
        "                                                 [row['Concepticon_Gloss.y']]), axis=1) \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dHDtwQiHFCf"
      },
      "source": [
        "**1.2. Associativity:**\n",
        "\n",
        "Associativity data was retrieved from SWOW (Small World of Words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl5s2KLHDsg1"
      },
      "source": [
        "#associativity\n",
        "def bidirectionality (row_a, row_b):\n",
        "  strength = 0\n",
        "  pair = \" \"\n",
        "  strength=((row_a['R1.Strength'] + row_b['R1.Strength'])/2)\n",
        "  pair=(row_a['pairs'])\n",
        "  return (strength, pair)\n",
        "\n",
        "def add_associativity (df):\n",
        "  #Make everything lowercase to make word pairs comparable\n",
        "  for i in swow['cue']: \n",
        "    if i == str:\n",
        "      swow['cue'] = swow['cue'].apply(str.lower)\n",
        "  for i in swow['response']: \n",
        "    if i == str:\n",
        "      swow['response'] = swow['response'].apply(str.lower)\n",
        "  #make word pairs\n",
        "  swow['pairs'] = list(zip(swow['cue'], swow['response']))\n",
        "  #join both dfs\n",
        "  supp_df = df.join(swow.set_index('pairs'), on='pairs') #joins both dfs by pairs column\n",
        "  supp_df = supp_df.reset_index()\n",
        "  #make associativity pairs bidirectional\n",
        "  \"\"\"\n",
        "  if associativity was computed for a pair (one, a) but also for the same pair in the inverse order (a, one) it \n",
        "  returns their association strengths mean. This is done because colexification data is already bidirectional.\n",
        "  \"\"\"\n",
        "  assoc_strength_list=[]\n",
        "  i=0\n",
        "  while i <len(supp_df['pairs']):\n",
        "    x = supp_df.loc[i]['cue']\n",
        "    y = supp_df.loc[i]['response']\n",
        "    if (supp_df.loc[(supp_df['cue'] == y) & (supp_df['response'] == x)]).empty:\n",
        "      assoc_strength_list.append(bidirectionality(supp_df.loc[i], supp_df.loc[i]))\n",
        "    else:\n",
        "      a = (supp_df.loc[(supp_df['cue'] == y) & (supp_df['response'] == x)].index[0])\n",
        "      assoc_strength_list.append(bidirectionality(supp_df.loc[i], supp_df.loc[a]))\n",
        "    i=i+1\n",
        "  # save the list in a df to join it with full colexification data\n",
        "  df_assoc_strength = pandas.DataFrame(assoc_strength_list, columns=['assoc_str', 'pairs']) \n",
        "  #join assoc strengths to colexification df\n",
        "  df = df.join(df_assoc_strength.set_index('pairs'), on='pairs') \n",
        "  df = df.dropna()\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oChnbvCzHJAW"
      },
      "source": [
        "**1.3. Word Length**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1gEcYn5HCEq"
      },
      "source": [
        "def add_word_length (df):\n",
        "  df['n_char'] = df['clics_form'].str.len()\n",
        "  return(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJFbvOoHOsH"
      },
      "source": [
        "# 2. Intralinguistic features\n",
        "To compute some of this features wordnet was used. From it just the most frequent synset was taken. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB8ZomR2OcSY"
      },
      "source": [
        "2.1. Grammatical features: part of speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXcvYzr4LUxw"
      },
      "source": [
        "def part_of_speech (x):\n",
        "  #uses pos of most frequent synset\n",
        "  if wn.synsets(x) != []:\n",
        "    x = (wn.synsets(x)[0].pos())\n",
        "    return x\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI-PDvu8O1y8"
      },
      "source": [
        "#which pos\n",
        "def add_pos (df):\n",
        "  df['pos.x'] = df['Concepticon_Gloss.x'].apply(part_of_speech)\n",
        "  df['pos.y'] = df['Concepticon_Gloss.y'].apply(part_of_speech)\n",
        "  df['pos_pairs'] = list(zip(df['pos.x'], df['pos.y']))\n",
        "  for i in range(len(df['pos.x'])):\n",
        "    if df['pos.x'].iloc[i] == None or df['pos.y'].iloc[i]== None:\n",
        "      df['pos_pairs'].iloc[i] = None\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9jXACSgHfd4"
      },
      "source": [
        "# same or different pos\n",
        "def add_compared_pos(df):\n",
        "  df['pos_same']=0\n",
        "  for i in range(len(df['pos.x'])):\n",
        "    if df['pos.x'].iloc[i] == None or df['pos.y'].iloc[i]== None:\n",
        "      df['pos_same'].iloc[i] = None\n",
        "    elif df['pos.x'].iloc[i] == df['pos.y'].iloc[i]:\n",
        "      df['pos_same'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['pos_same'].iloc[i] = 0\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwzvTkXkHmzF"
      },
      "source": [
        "2.2. Semantic features: semantic taxonomic similarity, semantic field and ontological entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzdBhxXL8pX"
      },
      "source": [
        "def taxonomic_similarity (x, y, output):\n",
        "  # we use path and wup similarity because they are the most straight forward to retrieve\n",
        "  # lichenstein requires same part of speech\n",
        "  # also possible res, lin and jcn similarity but require IC info\n",
        "\n",
        "  path_similarity = []\n",
        "  wup_similarity = []\n",
        "\n",
        "\n",
        "  if wn.synsets(x) != [] and wn.synsets(y) != []:  \n",
        "    synx = wn.synsets(x)[0]\n",
        "    syny = wn.synsets(x)[0]\n",
        "    for syny in wn.synsets(y):\n",
        "        if synx.path_similarity(syny) != None:\n",
        "          path_similarity.append(synx.path_similarity(syny))\n",
        "        if synx.wup_similarity(syny) != None:\n",
        "          wup_similarity.append(synx.wup_similarity(syny))\n",
        "\n",
        "\n",
        "      \n",
        "    if output == 'path_similarity':\n",
        "      if path_similarity != []:\n",
        "        return ((sum(path_similarity))/(len(path_similarity))) \n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    if output == 'wup_similarity':\n",
        "      if wup_similarity != []:\n",
        "        return ((sum(wup_similarity))/(len(wup_similarity))) \n",
        "      else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1b2xuJTQFKF"
      },
      "source": [
        "def add_wup_pairs(df):\n",
        "  wup_pairs = []\n",
        "  for i, row in df.iterrows():\n",
        "    wup_pairs.append(taxonomic_similarity(row['Concepticon_Gloss.x'], row['Concepticon_Gloss.y'], output='wup_similarity'))\n",
        "  df['wup_pairs'] = np.array(wup_pairs)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfpYLp0Q6ml"
      },
      "source": [
        "def add_path_pairs(df):\n",
        "  path_pairs = []\n",
        "  for i, row in df.iterrows():\n",
        "    path_pairs.append(taxonomic_similarity(row['Concepticon_Gloss.x'], row['Concepticon_Gloss.y'], output='path_similarity'))\n",
        "  df['path_pairs'] = np.array(path_pairs)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOmRrl5ERHF3"
      },
      "source": [
        "def add_ontological_pairs (df):\n",
        "  df['Ontological_pairs']=0\n",
        "  for i in range(len(df['Ontological_Category.x'])):\n",
        "    if df['Ontological_Category.x'].iloc[i] == None or df['Ontological_Category.y'].iloc[i]== None:\n",
        "      df['Ontological_pairs'].iloc[i] = None\n",
        "    elif df['Ontological_Category.x'].iloc[i] == df['Ontological_Category.y'].iloc[i]:\n",
        "      df['Ontological_pairs'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['Ontological_pairs'].iloc[i] = 0\n",
        "  return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYjM2fYISlTp"
      },
      "source": [
        "def add_semantic_pairs (df):\n",
        "  df['Semantic_pairs']=0\n",
        "  for i in range(len(df['Semantic_Field.x'])):\n",
        "    if df['Semantic_Field.x'].iloc[i] == None or df['Semantic_Field.y'].iloc[i]== None:\n",
        "      df['Semantic_pairs'].iloc[i] = None\n",
        "    elif df['Semantic_Field.x'].iloc[i] == df['Semantic_Field.y'].iloc[i]:\n",
        "      df['Semantic_pairs'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['Semantic_pairs'].iloc[i] = 0\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGZV66kWQG2d"
      },
      "source": [
        "2.3. phonetic features: phonetic similarity between two word-forms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0jNyjihMxpZ"
      },
      "source": [
        "def min_edit_distance (x, y):\n",
        "  #to explore ortographic difference between Form and Clics normalized form\n",
        "  from nltk import edit_distance\n",
        "  return edit_distance(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_3TQTzSKLg"
      },
      "source": [
        "def add_phonetic_similarity (df):\n",
        "  phonetic_similarity = []\n",
        "  for i, row in df.iterrows():\n",
        "    phonetic_similarity.append(min_edit_distance(row['Form.x'], row['clics_form']))\n",
        "  df['phonetic_pairs'] = np.array(phonetic_similarity)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwprdWyR3UP"
      },
      "source": [
        "# 3. Descriptive features \n",
        "\n",
        "needed for the analysis and data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf3vORwRxBqw"
      },
      "source": [
        "3.1. Mixed df with attested and unattested computer generated colexifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPeHG5_TTb8"
      },
      "source": [
        "def add_unattested (df):\n",
        "  #flag attested colexifications\n",
        "  df['colexifies'] = 1\n",
        "  #generate and add unattested colexifications (credits: Sara) \n",
        "  df_unattested = df.copy()\n",
        "  attested_col = list (df_unattested['pairs'])\n",
        "  # create list with single items\n",
        "  attested_col_l = [item for t in attested_col for item in t]\n",
        "  attested_col_l = list (set(attested_col_l)) # delete duplicates\n",
        "  len(attested_col_l)\n",
        "  import random\n",
        "  # We create unattested colexifications randomly picking elements from the list, and keeping only those that are not in the original dataframe.\n",
        "  unattested_col = []\n",
        "  totN = df.shape[0]\n",
        "  for i in range(totN):\n",
        "    el1 = random.choice(attested_col_l)\n",
        "    el2 = random.choice(attested_col_l)\n",
        "    while el1 == el2:\n",
        "      el2 = random.choice(attested_col_l)\n",
        "    supp_t = (el1, el2)\n",
        "    if supp_t not in set(attested_col): # take only tuple that is not in attested ones --> size not the same but almost\n",
        "      unattested_col.append(supp_t)\n",
        "      # fix difference in number i--\n",
        "  df_unattested_supp = pandas.DataFrame(unattested_col, columns =['Concepticon_Gloss.x', 'Concepticon_Gloss.y'])\n",
        "  df_unattested_supp['pairs'] = unattested_col\n",
        "  df_unattested_supp['colexifies'] = 0\n",
        "  #add unattested colex to df\n",
        "  mixed_df = pandas.concat([df, df_unattested_supp])\n",
        "  return mixed_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtnEazIVTH_u"
      },
      "source": [
        "3.2. Flag the colexifications as maintained or lost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y45DncqnTQS4"
      },
      "source": [
        "def add_status (old_variety, df):\n",
        "  oldest_variety_df = df[df['variety']== old_variety]\n",
        "  maintained_colex_df = pandas.DataFrame()\n",
        "  for i in oldest_variety_df['pairs']:\n",
        "    maintained_colex_i = (df[df['pairs'] == i])\n",
        "    j = 0\n",
        "    while j in range(len(maintained_colex_i['pairs'])):\n",
        "      maintained_colex_df = maintained_colex_df.append(maintained_colex_i.iloc[j])\n",
        "      j+=1\n",
        "\n",
        "  maintained_colex_df = maintained_colex_df[maintained_colex_df['variety'] != old_variety]\n",
        "  maintained_colex_df['maintained']=1\n",
        "\n",
        "\n",
        "  lost_colex_df = oldest_variety_df\n",
        "  i = 0\n",
        "  while i in range(len(maintained_colex_df['pairs'])):\n",
        "    lost_colex_df = lost_colex_df[lost_colex_df['pairs'] != maintained_colex_df['pairs'].iloc[i]]\n",
        "    i+=1\n",
        "  lost_colex_df['maintained']=0\n",
        "  mixed_df = pandas.concat([lost_colex_df, maintained_colex_df])\n",
        "  return mixed_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znbqwk8EThUv"
      },
      "source": [
        "# 4. Add all features to the dfs, save and download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jCisv-bxZQb"
      },
      "source": [
        "def add_features (old_variety, df):\n",
        "  lowercase(df)\n",
        "  add_colex_pairs(df)\n",
        "  df = add_status(old_variety, df)\n",
        "  add_phonetic_similarity(df)\n",
        "  df = add_unattested(df)\n",
        "  add_associativity(df)\n",
        "  add_cosine_sim(df)\n",
        "  add_word_length(df)\n",
        "  add_pos(df)\n",
        "  add_compared_pos(df)\n",
        "  add_path_pairs(df)\n",
        "  add_wup_pairs(df)\n",
        "  add_semantic_pairs(df)\n",
        "  add_ontological_pairs(df)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iEEH4YaTpvl"
      },
      "source": [
        " def save_and_download (df, csv_name): \n",
        "  df.to_csv(csv_name)\n",
        "  files.download(csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExUqHdjGy3cJ"
      },
      "source": [
        "add_features('Proto Polynesian', polynesian_df)\n",
        "save_and_download(polynesian_df, 'polynesian_df_features.csv')\n",
        "add_features('Latin', romance_df)\n",
        "save_and_download(romance_df, 'romance_df_features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}