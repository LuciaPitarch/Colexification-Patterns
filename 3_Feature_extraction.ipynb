{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Feature_extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIyKN2+5LE06ptDxoRyhNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuciaPitarch/Colexification-Patterns/blob/main/3_Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6VMzE7VGZfc"
      },
      "source": [
        "# 0. Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVA4ebnjGK3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03364e3-c085-404b-e0e1-0c4c7e03015e"
      },
      "source": [
        "# Import libraries\n",
        "import pandas\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')\n",
        "# download pre-trained embedding model from nltk library\n",
        "nltk.download('word2vec_sample')\n",
        "path_to_word2vec_sample = nltk.data.find('models/word2vec_sample/pruned.word2vec.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYR-C0W-GUXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9a39a9a-eb02-46f4-9d74-e7730efd991d"
      },
      "source": [
        "# Import csv files\n",
        "!gdown --id 1GOhH20R9ToDnsoCqO0MjX0NUVSmZ2vqG #polynesian colexifications df\n",
        "!gdown --id 1HYTxiQMrpz_IYCsXoZIYFoH54mdkbixR #romance colexifications df\n",
        "!gdown --id 1YJz8XHfCn6_HGR7gbRwB2Ry2Z4ZSvYWK #swow_strength"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GOhH20R9ToDnsoCqO0MjX0NUVSmZ2vqG\n",
            "To: /content/polynesian_df_colexifications.csv\n",
            "100% 515k/515k [00:00<00:00, 33.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HYTxiQMrpz_IYCsXoZIYFoH54mdkbixR\n",
            "To: /content/romance_df_colexifications.csv\n",
            "100% 256k/256k [00:00<00:00, 74.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YJz8XHfCn6_HGR7gbRwB2Ry2Z4ZSvYWK\n",
            "To: /content/st_swow.csv\n",
            "18.5MB [00:00, 86.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yBbY0Z4Ggts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e324611-e039-480f-d821-766faf73f1b5"
      },
      "source": [
        "# Read files\n",
        "polynesian_df = pandas.read_csv('polynesian_df_colexifications.csv')\n",
        "romance_df = pandas.read_csv('romance_df_colexifications.csv')\n",
        "swow = pandas.read_csv('st_swow.csv', delimiter=(';'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNycCBUTG5ji"
      },
      "source": [
        "First lowercase words and make word pairs which will be needed to compute some of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0zq27tJGxUT"
      },
      "source": [
        "def lowercase (df):\n",
        "  df['Concepticon_Gloss.x'] = df['Concepticon_Gloss.x'].str.lower()\n",
        "  df['Concepticon_Gloss.y'] = df['Concepticon_Gloss.y'].str.lower()\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UNj_wZYGzF9"
      },
      "source": [
        "def add_colex_pairs (df):\n",
        "  df['pairs'] = list(zip(df['Concepticon_Gloss.x'], df['Concepticon_Gloss.y']))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJFbvOoHOsH"
      },
      "source": [
        "# 1. Features\n",
        "To compute some of this features wordnet was used. From it just the most frequent synset was taken. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB8ZomR2OcSY"
      },
      "source": [
        "#1.1.  Grammatical features: part of speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXcvYzr4LUxw"
      },
      "source": [
        "def part_of_speech (x):\n",
        "  #uses pos of most frequent synset\n",
        "  if wn.synsets(x) != []:\n",
        "    x = (wn.synsets(x)[0].pos())\n",
        "    return x\n",
        "  else:\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI-PDvu8O1y8"
      },
      "source": [
        "#which pos\n",
        "def add_pos (df):\n",
        "  df['pos.x'] = df['Concepticon_Gloss.x'].apply(part_of_speech)\n",
        "  df['pos.y'] = df['Concepticon_Gloss.y'].apply(part_of_speech)\n",
        "  df['pos_pairs'] = list(zip(df['pos.x'], df['pos.y']))\n",
        "  for i in range(len(df['pos.x'])):\n",
        "    if df['pos.x'].iloc[i] == None or df['pos.y'].iloc[i]== None:\n",
        "      df['pos_pairs'].iloc[i] = None\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9jXACSgHfd4"
      },
      "source": [
        "# same or different pos\n",
        "def add_compared_pos(df):\n",
        "  df['pos_same']=0\n",
        "  for i in range(len(df['pos.x'])):\n",
        "    if df['pos.x'].iloc[i] == None or df['pos.y'].iloc[i]== None:\n",
        "      df['pos_same'].iloc[i] = None\n",
        "    elif df['pos.x'].iloc[i] == df['pos.y'].iloc[i]:\n",
        "      df['pos_same'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['pos_same'].iloc[i] = 0\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwzvTkXkHmzF"
      },
      "source": [
        "# 1.2. Semantic features: \n",
        "cosine similarity, semantic taxonomic similarity, semantic field and ontological entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6YNe0fFKmEt"
      },
      "source": [
        "#cosine similarity\n",
        "words = []\n",
        "vectors = []\n",
        "word2vec = {}\n",
        "with open(path_to_word2vec_sample) as file:\n",
        "    for i, line in enumerate(file):\n",
        "        if i == 0:\n",
        "            continue\n",
        "        line = line.split()\n",
        "        word = line[0]\n",
        "        vector = line[1:]\n",
        "        vector = [float(s) for s in vector]\n",
        "        \n",
        "        words.append(word)\n",
        "        vectors.append(vector)\n",
        "        word2vec[word] = vector\n",
        "  \n",
        "def norm(vector):\n",
        "    return np.sqrt(sum([a*a for a in vector]))\n",
        "\n",
        "def dot(vector1, vector2):\n",
        "    return sum([i * j for i, j in zip(vector1, vector2)])\n",
        "\n",
        "def cosine(vector1, vector2):\n",
        "    return dot(vector1, vector2) / (norm(vector1) * norm(vector2))\n",
        "\n",
        "def concept_to_embedding (concept1, concept2):\n",
        "  if concept1[0] in word2vec and concept2[0] in word2vec:\n",
        "    return cosine(word2vec[concept1[0]], word2vec[concept2[0]])\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def add_cosine_sim(df):\n",
        "  df['cosine_sim'] = df.apply(lambda row: concept_to_embedding([row['Concepticon_Gloss.x']], \n",
        "                                                 [row['Concepticon_Gloss.y']]), axis=1) \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HzdBhxXL8pX"
      },
      "source": [
        "def taxonomic_similarity (x, y, output):\n",
        "  # we use path and wup similarity because they are the most straight forward to retrieve\n",
        "  # lichenstein requires same part of speech\n",
        "  # also possible res, lin and jcn similarity but require IC info\n",
        "\n",
        "  path_similarity = []\n",
        "  wup_similarity = []\n",
        "\n",
        "\n",
        "  if wn.synsets(x) != [] and wn.synsets(y) != []:  \n",
        "    synx = wn.synsets(x)[0]\n",
        "    syny = wn.synsets(x)[0]\n",
        "    for syny in wn.synsets(y):\n",
        "        if synx.path_similarity(syny) != None:\n",
        "          path_similarity.append(synx.path_similarity(syny))\n",
        "        if synx.wup_similarity(syny) != None:\n",
        "          wup_similarity.append(synx.wup_similarity(syny))\n",
        "\n",
        "\n",
        "      \n",
        "    if output == 'path_similarity':\n",
        "      if path_similarity != []:\n",
        "        return ((sum(path_similarity))/(len(path_similarity))) \n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    if output == 'wup_similarity':\n",
        "      if wup_similarity != []:\n",
        "        return ((sum(wup_similarity))/(len(wup_similarity))) \n",
        "      else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1b2xuJTQFKF"
      },
      "source": [
        "def add_wup_pairs(df):\n",
        "  wup_pairs = []\n",
        "  for i, row in df.iterrows():\n",
        "    wup_pairs.append(taxonomic_similarity(row['Concepticon_Gloss.x'], row['Concepticon_Gloss.y'], output='wup_similarity'))\n",
        "  df['wup_pairs'] = np.array(wup_pairs)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfpYLp0Q6ml"
      },
      "source": [
        "def add_path_pairs(df):\n",
        "  path_pairs = []\n",
        "  for i, row in df.iterrows():\n",
        "    path_pairs.append(taxonomic_similarity(row['Concepticon_Gloss.x'], row['Concepticon_Gloss.y'], output='path_similarity'))\n",
        "  df['path_pairs'] = np.array(path_pairs)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOmRrl5ERHF3"
      },
      "source": [
        "def add_ontological_pairs (df):\n",
        "  df['Ontological_pairs']=0\n",
        "  for i in range(len(df['Ontological_Category.x'])):\n",
        "    if df['Ontological_Category.x'].iloc[i] == None or df['Ontological_Category.y'].iloc[i]== None:\n",
        "      df['Ontological_pairs'].iloc[i] = None\n",
        "    elif df['Ontological_Category.x'].iloc[i] == df['Ontological_Category.y'].iloc[i]:\n",
        "      df['Ontological_pairs'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['Ontological_pairs'].iloc[i] = 0\n",
        "  return df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYjM2fYISlTp"
      },
      "source": [
        "def add_semantic_pairs (df):\n",
        "  df['Semantic_pairs']=0\n",
        "  for i in range(len(df['Semantic_Field.x'])):\n",
        "    if df['Semantic_Field.x'].iloc[i] == None or df['Semantic_Field.y'].iloc[i]== None:\n",
        "      df['Semantic_pairs'].iloc[i] = None\n",
        "    elif df['Semantic_Field.x'].iloc[i] == df['Semantic_Field.y'].iloc[i]:\n",
        "      df['Semantic_pairs'].iloc[i] = 1 \n",
        "    else:\n",
        "      df['Semantic_pairs'].iloc[i] = 0\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGZV66kWQG2d"
      },
      "source": [
        "#1.3. phonetic features: \n",
        "word length and phonetic similarity between two word-forms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0jNyjihMxpZ"
      },
      "source": [
        "def min_edit_distance (x, y):\n",
        "  #to explore ortographic difference between Form and Clics normalized form\n",
        "  from nltk import edit_distance\n",
        "  return edit_distance(x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1gEcYn5HCEq"
      },
      "source": [
        "def add_word_length (df):\n",
        "  df['n_char'] = df['clics_form'].str.len()\n",
        "  return(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_3TQTzSKLg"
      },
      "source": [
        "def add_phonetic_similarity (df):\n",
        "  phonetic_similarity = []\n",
        "  for i, row in df.iterrows():\n",
        "    phonetic_similarity.append(min_edit_distance(row['Form.x'], row['clics_form']))\n",
        "  df['phonetic_pairs'] = np.array(phonetic_similarity)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwprdWyR3UP"
      },
      "source": [
        "# 1.4. Descriptive features \n",
        "\n",
        "needed for the analysis and data visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf3vORwRxBqw"
      },
      "source": [
        "Mixed df with attested and unattested computer generated colexifications"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRPeHG5_TTb8"
      },
      "source": [
        "def add_unattested (df):\n",
        "  #flag attested colexifications\n",
        "  df['colexifies'] = 1\n",
        "  #generate and add unattested colexifications (credits: Sara) \n",
        "  df_unattested = df.copy()\n",
        "  attested_col = list (df_unattested['pairs'])\n",
        "  # create list with single items\n",
        "  attested_col_l = [item for t in attested_col for item in t]\n",
        "  attested_col_l = list (set(attested_col_l)) # delete duplicates\n",
        "  len(attested_col_l)\n",
        "  import random\n",
        "  # We create unattested colexifications randomly picking elements from the list, and keeping only those that are not in the original dataframe.\n",
        "  unattested_col = []\n",
        "  totN = df.shape[0]\n",
        "  for i in range(totN):\n",
        "    el1 = random.choice(attested_col_l)\n",
        "    el2 = random.choice(attested_col_l)\n",
        "    while el1 == el2:\n",
        "      el2 = random.choice(attested_col_l)\n",
        "    supp_t = (el1, el2)\n",
        "    if supp_t not in set(attested_col): # take only tuple that is not in attested ones --> size not the same but almost\n",
        "      unattested_col.append(supp_t)\n",
        "      # fix difference in number i--\n",
        "  df_unattested_supp = pandas.DataFrame(unattested_col, columns =['Concepticon_Gloss.x', 'Concepticon_Gloss.y'])\n",
        "  df_unattested_supp['pairs'] = unattested_col\n",
        "  df_unattested_supp['colexifies'] = 0\n",
        "  #add unattested colex to df\n",
        "  mixed_df = pandas.concat([df, df_unattested_supp])\n",
        "  return mixed_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtnEazIVTH_u"
      },
      "source": [
        "Flag the colexifications as maintained or lost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y45DncqnTQS4"
      },
      "source": [
        "def add_status (old_variety, df):\n",
        "  oldest_variety_df = df[df['variety']== old_variety]\n",
        "  maintained_colex_df = pandas.DataFrame()\n",
        "  for i in oldest_variety_df['pairs']:\n",
        "    maintained_colex_i = (df[df['pairs'] == i])\n",
        "    j = 0\n",
        "    while j in range(len(maintained_colex_i['pairs'])):\n",
        "      maintained_colex_df = maintained_colex_df.append(maintained_colex_i.iloc[j])\n",
        "      j+=1\n",
        "\n",
        "  maintained_colex_df = maintained_colex_df[maintained_colex_df['variety'] != old_variety]\n",
        "  maintained_colex_df['maintained']=1\n",
        "\n",
        "\n",
        "  lost_colex_df = oldest_variety_df\n",
        "  i = 0\n",
        "  while i in range(len(maintained_colex_df['pairs'])):\n",
        "    lost_colex_df = lost_colex_df[lost_colex_df['pairs'] != maintained_colex_df['pairs'].iloc[i]]\n",
        "    i+=1\n",
        "  lost_colex_df['maintained']=0\n",
        "  mixed_df = pandas.concat([lost_colex_df, maintained_colex_df])\n",
        "  return mixed_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znbqwk8EThUv"
      },
      "source": [
        "# 2. Add all features to the dfs, save and download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jCisv-bxZQb"
      },
      "source": [
        "def add_features (old_variety, df):\n",
        "  lowercase(df)\n",
        "  add_colex_pairs(df)\n",
        "  df = add_status(old_variety, df)\n",
        "  add_phonetic_similarity(df)\n",
        "  df = add_unattested(df)\n",
        "  # add_associativity(df) # arreglar\n",
        "  add_cosine_sim(df)\n",
        "  add_word_length(df)\n",
        "  add_pos(df)\n",
        "  add_compared_pos(df)\n",
        "  add_path_pairs(df)\n",
        "  add_wup_pairs(df)\n",
        "  add_semantic_pairs(df)\n",
        "  add_ontological_pairs(df)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iEEH4YaTpvl"
      },
      "source": [
        " def save_and_download (df, csv_name): \n",
        "  df.to_csv(csv_name)\n",
        "  files.download(csv_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExUqHdjGy3cJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "bbb396c9-57f1-4b98-b490-133cb02acdfa"
      },
      "source": [
        "polynesian_df = add_features('Proto Polynesian', polynesian_df)\n",
        "save_and_download(polynesian_df, 'polynesian_df_features.csv')\n",
        "romance_df = add_features('Latin', romance_df)\n",
        "save_and_download(romance_df, 'romance_df_features.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_87262369-74c0-4b6a-a6c0-842561eaf130\", \"polynesian_df_features.csv\", 317362)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_40f6dac4-2530-4098-ab69-36e59f0e6b09\", \"romance_df_features.csv\", 348485)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}